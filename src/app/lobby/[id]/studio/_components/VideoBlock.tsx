import { useTimeStore } from "@/app/_store/TimeStore";
import React, { useEffect, useRef } from "react";

interface VideoBlockProps {
  videoUrl: string | undefined;
  videoRef: React.RefObject<VideoElementWithCapturestream | null>;
  isMuted: boolean;
  isProcessedAudio: boolean;
}

const VideoBlock = ({
  videoUrl,
  videoRef,
  isMuted,
  isProcessedAudio,
}: VideoBlockProps) => {
  const { isPlaying, time } = useTimeStore();
  const audioContextRef = useRef<AudioContext | null>(null);
  const processedBufferRef = useRef<AudioBuffer | null>(null);
  const audioSourceRef = useRef<AudioBufferSourceNode | null>(null);
  const gainNodeRef = useRef<GainNode | null>(null);
  useEffect(() => {
    if (isMuted) {
      console.log("[Video Block] ë¹„ë””ì˜¤ ìŒì†Œê±° O");
    } else {
      console.log("[Video Block] ë¹„ë””ì˜¤ ìŒì†Œê±° X");
    }

    if (isProcessedAudio) {
      console.log("[Video Block] ë³´ì»¬ ì œê±° O");
    } else {
      console.log("[Video Block] ë³´ì»¬ ì œê±° X");
    }
  }, [isMuted, isProcessedAudio]);
  useEffect(() => {
    if (!videoUrl) return;
    const initAudio = async () => {
      if (!audioContextRef.current) {
        audioContextRef.current = new AudioContext();
      }

      try {
        const response = await fetch(videoUrl);
        const arrayBuffer = await response.arrayBuffer();
        const audioBuffer =
          await audioContextRef.current.decodeAudioData(arrayBuffer);

        // âœ… ë³´ì»¬ ì œê±°ëœ ì˜¤ë””ì˜¤ë¥¼ ì´ˆê¸° ì„¤ì •
        removeVocals(audioBuffer);
      } catch (error) {
        console.error("âŒ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨:", error);
      }
    };

    initAudio();
  }, [videoUrl]);

  useEffect(() => {
    if (!videoRef.current) return;

    if (isProcessedAudio) {
      videoRef.current.muted = true;
    } else {
      videoRef.current.muted = isMuted;
    }
  }, [isProcessedAudio, isMuted]);

  useEffect(() => {
    if (!videoRef.current) return;

    // ë¶€ëª¨ì—ì„œ ì „ë‹¬ë°›ì€ ìƒíƒœì— ë”°ë¼ ì¬ìƒ/ì •ì§€
    if (isPlaying) {
      videoRef.current
        .play()
        .catch((error) => console.error("ë¹„ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨: ", error));
      if (isProcessedAudio) {
        handlePlayAudio();
      }
    } else {
      videoRef.current.pause();
      if (isProcessedAudio) {
        handleStopAudio();
      }
    }
  }, [isPlaying]);

  useEffect(() => {
    if (!videoRef.current) return;

    // íƒ€ì„ë¼ì¸ ì´ë™ ë™ê¸°í™”
    if (Math.abs(videoRef.current.currentTime - time) > 0.5) {
      videoRef.current.currentTime = time;
    }
  }, [time]);

  // âœ… ë³´ì»¬ ì œê±° í•¨ìˆ˜ (ë Œë”ë§ ì‹œ ìë™ ì‹¤í–‰)
  const removeVocals = (buffer: AudioBuffer) => {
    if (!audioContextRef.current) {
      console.error("âŒ AudioContextê°€ ì—†ìŠµë‹ˆë‹¤.");
      return;
    }

    const numOfChannels = buffer.numberOfChannels;
    if (numOfChannels < 2) {
      console.error("âŒ ìŠ¤í…Œë ˆì˜¤ ì˜¤ë””ì˜¤ê°€ ì•„ë‹™ë‹ˆë‹¤. ë³´ì»¬ ì œê±° ë¶ˆê°€ëŠ¥.");
      return;
    }

    const leftChannel = buffer.getChannelData(0);
    const rightChannel = buffer.getChannelData(1);
    const length = leftChannel.length;

    const newBuffer = audioContextRef.current.createBuffer(
      2,
      length,
      buffer.sampleRate,
    );
    const newLeftChannel = newBuffer.getChannelData(0);
    const newRightChannel = newBuffer.getChannelData(1);

    for (let i = 0; i < length; i++) {
      const center = (leftChannel[i] - rightChannel[i]) / 2; // ë³´ì»¬ ì œê±°
      newLeftChannel[i] = center;
      newRightChannel[i] = center;
    }

    console.log("âœ… ë³´ì»¬ ì œê±°ëœ ì˜¤ë””ì˜¤ ë²„í¼ ìƒì„± ì™„ë£Œ");
    processedBufferRef.current = newBuffer;
  };

  const handlePlayAudio = () => {
    if (!audioContextRef.current) {
      console.error("âŒ AudioContextê°€ ì—†ìŠµë‹ˆë‹¤.");
      return;
    }

    const bufferToPlay = processedBufferRef.current;

    if (!bufferToPlay) {
      console.error("âŒ ì¬ìƒí•  ì˜¤ë””ì˜¤ ë²„í¼ê°€ ì—†ìŠµë‹ˆë‹¤.");
      return;
    }

    // ê¸°ì¡´ ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ì§€
    if (audioSourceRef.current) {
      audioSourceRef.current.stop();
    }

    const source = audioContextRef.current.createBufferSource();
    source.buffer = bufferToPlay;

    // âœ… GainNode ì¶”ê°€ (ìŒì†Œê±°/ë³¼ë¥¨ ì¡°ì ˆ)
    if (!gainNodeRef.current) {
      gainNodeRef.current = audioContextRef.current.createGain();
    }

    source.connect(gainNodeRef.current);
    gainNodeRef.current.connect(audioContextRef.current.destination);

    // âœ… ìŒì†Œê±° ìƒíƒœ ë°˜ì˜
    gainNodeRef.current.gain.value = isMuted ? 0 : 1;

    const startTime = videoRef.current?.currentTime;
    source.start(0, startTime);
    source.playbackRate.value = videoRef.current?.playbackRate || 1;

    audioSourceRef.current = source;
    console.log(
      `ğŸµ ${isProcessedAudio ? "ë³´ì»¬ ì œê±°ëœ" : "ì›ë³¸"} ì˜¤ë””ì˜¤ ${startTime}ì´ˆë¶€í„° ì¬ìƒ`,
    );
  };

  const handleStopAudio = () => {
    if (audioSourceRef.current) {
      audioSourceRef.current.stop(); // âœ… ê¸°ì¡´ ì˜¤ë””ì˜¤ ì •ì§€
      audioSourceRef.current.disconnect();
      audioSourceRef.current = null;
    }

    if (gainNodeRef.current) {
      gainNodeRef.current.disconnect();
      gainNodeRef.current = null;
    }

    console.log("ğŸ”‡ ë³´ì»¬ ì œê±°ëœ ì˜¤ë””ì˜¤ ì •ì§€");
  };

  return (
    <div className="flex flex-col items-center">
      <video ref={videoRef} src={videoUrl} className="max-h-[407px]" />
    </div>
  );
};

export default VideoBlock;
