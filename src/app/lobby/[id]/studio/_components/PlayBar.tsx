import { toast } from "sonner";
import { useParams } from "next/navigation";
import React, { useEffect, useRef, useState } from "react";

import { useMicStore } from "@/app/_store/MicStore";
import { useUserStore } from "@/app/_store/UserStore";
import { useTimeStore } from "@/app/_store/TimeStore";
import { useRecordingStore } from "@/app/_store/RecordingStore";

import { postAsset } from "@/app/_apis/studio";
import { Asset, Track } from "@/app/_types/studio";
import { formatTime } from "@/app/_utils/formatTime";

import H4 from "@/app/_components/H4";
import ShareButton from "./ShareButton";
import StoreButton from "./StoreButton";
import RenderingButton from "./RenderingButton";

import RecordButton from "@/public/images/icons/icon-record.svg";
import PlayButton from "@/public/images/icons/icon-play.svg";
import StopButton from "@/public/images/icons/icon-stop.svg";
import PauseButton from "@/public/images/icons/icon-pause.svg";
import { usePlaySocket } from "@/app/_hooks/usePlaySocket";

interface PlayBarProps {
  videoRef: React.RefObject<VideoElementWithCapturestream | null>;
  videoUrl: string | undefined;
  duration: number;
  setDuration: React.Dispatch<React.SetStateAction<number>>;
  tracks: Track[];
  setTracks: React.Dispatch<React.SetStateAction<Track[]>>;
  assets: Asset[];
}

const PlayBar = ({
  videoRef,
  videoUrl,
  duration,
  setDuration,
  tracks,
  setTracks,
  assets,
}: PlayBarProps) => {
  const { self } = useUserStore();
  const { micStatus } = useMicStore();
  const { time, isPlaying, play, pause, reset } = useTimeStore();
  const {
    isRecording,
    audioContext,
    startRecording,
    stopRecording,
    createAudioFile,
    setMediaRecorder,
    setAudioContext,
    setAnalyser,
  } = useRecordingStore();

  const { sendPlaybackStatus } = usePlaySocket();

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const userId = self?.memberId ?? null;

  const params = useParams();
  const pid = params.id;

  const isManualRecording = useRef(false); // ðŸ”¥ ì‚¬ìš©ìžê°€ ì§ì ‘ ë…¹ìŒ ë²„íŠ¼ì„ ëˆŒë €ëŠ”ì§€ ì¶”ì 

  // useEffect(): isRecording ì†Œì¼“ ê°ì§€ ë° ìžë™ ë…¹ìŒ ìž¬ìƒ / ì •ì§€
  useEffect(() => {
    console.log("ðŸ”„ `useEffect` ê°ì§€ - isRecording ë³€ê²½ë¨:", isRecording);

    if (!isManualRecording.current) {
      if (isRecording) {
        console.log("ðŸ”¥ ì†Œì¼“ì—ì„œ ë°›ì€ recordingìœ¼ë¡œ ë…¹ìŒ ì‹œìž‘");
        startRecordingFromSocket(); // ðŸŽ¯ ìƒˆë¡œìš´ ë…¹ìŒ í•¨ìˆ˜ í˜¸ì¶œ
      } else {
        console.log("ðŸ”¥ ì†Œì¼“ì—ì„œ ë°›ì€ recordingìœ¼ë¡œ ë…¹ìŒ ì •ì§€");
        stopRecordingFromSocket();
      }
    }
  }, [isRecording]);

  // useEffect: ë™ì˜ìƒ ê¸¸ì´ ì´ˆê³¼ì‹œ, ìžë™ ì •ì§€ (ë…¹ìŒì‹œ, ë…¹ìŒë„ ì •ì§€)
  useEffect(() => {
    if (time >= duration) {
      if (isRecording) stopRecording();
      pause();
      reset();
    }
  }, [time, duration]);

  // useEffect: SpaceBar -> ìž¬ìƒ / ì¼ì‹œ ì •ì§€
  useEffect(() => {
    const handleKeyDown = (event: KeyboardEvent) => {
      const activeElement = document.activeElement;
      if (
        activeElement &&
        ["INPUT", "TEXTAREA"].includes(activeElement.tagName)
      ) {
        return;
      }
      if (event.code === "Space") {
        event.preventDefault();

        if (isPlaying) {
          sendPlaybackStatus({
            recording: isRecording,
            playState: "STOP",
            timelineMarker: 0,
          });
        } else {
          sendPlaybackStatus({
            recording: isRecording,
            playState: "PLAY",
            timelineMarker: 0,
          });
        }
      }
    };

    window.addEventListener("keydown", handleKeyDown);
    return () => {
      window.removeEventListener("keydown", handleKeyDown);
    };
  }, [isPlaying, play, pause]);

  // useEffect: ë™ì˜ìƒ ê¸¸ì´ì— ë§žê²Œ ì „ì²´ duration ì„¤ì •
  useEffect(() => {
    const videoElement = videoRef.current;

    if (!videoElement) return; // videoRefê°€ ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ

    const handleMetadataLoaded = () => {
      setDuration(videoElement.duration || 0);
      console.log(
        "ðŸ“Œ ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ë¡œë“œë¨, duration:",
        videoElement.duration,
      );
    };

    // ðŸŽ¯ ë¹„ë””ì˜¤ì˜ `loadedmetadata` ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•˜ì—¬ `duration`ì„ ì„¤ì •
    videoElement.addEventListener("loadedmetadata", handleMetadataLoaded);

    // ðŸŽ¯ cleanup í•¨ìˆ˜ì—ì„œ ì´ë²¤íŠ¸ ì œê±°
    return () => {
      videoElement.removeEventListener("loadedmetadata", handleMetadataLoaded);
    };
  }, [videoRef]);

  // handleRecording(): ë…¹ìŒí•˜ëŠ” í•¨ìˆ˜
  const handleRecording = async () => {
    console.log("ðŸŽ¤ handleRecording ì‹¤í–‰ë¨! í˜„ìž¬ isRecording:", isRecording);

    if (!userId) {
      toast.warning("ì˜¤ë¥˜: ì‚¬ìš©ìž ì •ë³´ê°€ ì—†ì–´, ë…¹ìŒì„ ì‹œìž‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
      return;
    }

    sendPlaybackStatus({
      recording: !isRecording,
      playState: isRecording ? "STOP" : "PLAY",
      timelineMarker: isRecording ? 0 : time,
    });

    if (isRecording) {
      console.log("ðŸ›‘ ë…¹ìŒ ì¤‘ì§€ ì²˜ë¦¬ ì¤‘...");
      mediaRecorderRef.current?.stop();
      stopRecording();

      if (audioContext) {
        audioContext.close();
        setAudioContext(null);
        setAnalyser(null);
      }

      setMediaRecorder(null);
      isManualRecording.current = false; // ðŸ”¥ ë…¹ìŒ ì¢…ë£Œ í›„ í”Œëž˜ê·¸ ì´ˆê¸°í™”
      return;
    }

    console.log("ðŸŽ¬ ë…¹ìŒ ì‹œìž‘!");
    isManualRecording.current = true; // ðŸ”¥ ì‚¬ìš©ìžê°€ ì§ì ‘ ì‹¤í–‰í•œ ë…¹ìŒ

    const currentTime = time;
    const activeMics = Object.entries(micStatus)
      .filter(([_, isOn]) => isOn)
      .map(([userId]) => userId);

    if (activeMics.length === 0) {
      toast.warning("ì—­í•  íƒ­ì—ì„œ ìžì‹ ì˜ ë§ˆì´í¬ë¥¼ ì¼œì£¼ì„¸ìš”!");
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const recorder = new MediaRecorder(stream);
      mediaRecorderRef.current = recorder;

      const chunks: Blob[] = [];
      recorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunks.push(event.data);
        }
      };

      const track = tracks.find((t) => t.recorderId === userId);
      if (!track) {
        toast.warning("ì˜¤ë””ì˜¤ íŠ¸ëž™ì— ì°¸ì—¬ìžë¥¼ í• ë‹¹í•´ì£¼ì„¸ìš”!");
        return;
      }

      recorder.onstop = async () => {
        toast.success("ë…¹ìŒëœ íŒŒì¼ì„ ì €ìž¥ ì¤‘ìž…ë‹ˆë‹¤...");
        const audioBlob = new Blob(chunks, { type: "audio/wav" });
        const url = URL.createObjectURL(audioBlob);
        console.log("ðŸŽµ ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ URL:", url);

        if (!track.recorderId) {
          toast.error("íŠ¸ëž™ì— í• ë‹¹ëœ ì°¸ì—¬ìžê°€ ì—†ìŠµë‹ˆë‹¤.");
          return;
        }
        const newUrl = await postAsset(String(pid), audioBlob);
        createAudioFile(track.trackId, newUrl, currentTime);
      };

      recorder.start();
      startRecording(track.trackId);
      setMediaRecorder(recorder);

      const AudioCtx = window.AudioContext;
      const audioCtx = new AudioCtx();
      const source = audioCtx.createMediaStreamSource(stream);
      const analyser = audioCtx.createAnalyser();
      analyser.fftSize = 2048;
      source.connect(analyser);

      setAudioContext(audioCtx);
      setAnalyser(analyser);
    } catch (error) {
      toast.error(`ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ${error}`);
    }
  };

  // startRecordingFromSocket(): ì†Œì¼“ ìƒíƒœ ë°›ì•„ì„œ ìžë™ ë…¹ìŒ ì§„í–‰
  const startRecordingFromSocket = async () => {
    console.log("ðŸŽ¬ [ì†Œì¼“] ë…¹ìŒ ì‹œìž‘ - isRecording ìƒíƒœ:", isRecording);

    if (!userId) {
      toast.warning("ì˜¤ë¥˜: ì‚¬ìš©ìž ì •ë³´ê°€ ì—†ì–´ ë…¹ìŒì„ ì‹œìž‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
      return;
    }

    const currentTime = time;
    const activeMics = Object.entries(micStatus)
      .filter(([_, isOn]) => isOn)
      .map(([userId]) => userId);

    if (activeMics.length === 0) {
      toast.warning("ì—­í•  íƒ­ì—ì„œ ìžì‹ ì˜ ë§ˆì´í¬ë¥¼ ì¼œì£¼ì„¸ìš”!");
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const recorder = new MediaRecorder(stream);
      mediaRecorderRef.current = recorder;

      const chunks: Blob[] = [];
      recorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunks.push(event.data);
        }
      };

      const track = tracks.find((t) => t.recorderId === userId);
      if (!track) {
        toast.warning("ì˜¤ë””ì˜¤ íŠ¸ëž™ì— ì°¸ì—¬ìžë¥¼ í• ë‹¹í•´ì£¼ì„¸ìš”!");
        return;
      }

      recorder.onstop = async () => {
        toast.success("ë…¹ìŒëœ íŒŒì¼ì„ ì €ìž¥ ì¤‘ìž…ë‹ˆë‹¤...");
        const audioBlob = new Blob(chunks, { type: "audio/webm" });
        const url = URL.createObjectURL(audioBlob);
        console.log("ðŸŽµ ìƒì„±ëœ ì˜¤ë””ì˜¤ íŒŒì¼ URL:", url);

        if (!track.recorderId) {
          toast.error("íŠ¸ëž™ì— í• ë‹¹ëœ ì°¸ì—¬ìžê°€ ì—†ìŠµë‹ˆë‹¤.");
          return;
        }

        const newUrl = await postAsset(String(pid), audioBlob);
        createAudioFile(track.trackId, newUrl, currentTime);
      };

      recorder.start();
      startRecording(track.trackId);
      setMediaRecorder(recorder);

      // ê¸°ì¡´ try ë¸”ë¡ì˜ AudioContext ê´€ë ¨ ë¡œì§ë„ í¬í•¨
      const AudioCtx = window.AudioContext;
      const audioCtx = new AudioCtx();
      const source = audioCtx.createMediaStreamSource(stream);
      const analyser = audioCtx.createAnalyser();
      analyser.fftSize = 2048;
      source.connect(analyser);

      setAudioContext(audioCtx);
      setAnalyser(analyser);
    } catch (error) {
      toast.error(`ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ${error}`);
    }
  };

  // stopRecordingFromSocket(): ì†Œì¼“ ìƒíƒœ ë°›ì•„ ìžë™ ë…¹ìŒ ì •ì§€
  const stopRecordingFromSocket = () => {
    console.log("ðŸ›‘ [ì†Œì¼“] ë…¹ìŒ ì¤‘ì§€ ì‹¤í–‰ë¨!");

    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop();
    }

    stopRecording();

    if (audioContext) {
      audioContext.close();
      setAudioContext(null);
      setAnalyser(null);
    }

    setMediaRecorder(null);
  };

  // handlePlayButton(): ìž¬ìƒ/ì¼ì‹œì •ì§€ ë²„íŠ¼ í´ë¦­ í•¨ìˆ˜
  const handlePlayButton = () => {
    if (isPlaying) {
      sendPlaybackStatus({
        recording: false,
        playState: "PAUSE",
        timelineMarker: time,
      });
    } else {
      sendPlaybackStatus({
        recording: false,
        playState: "PLAY",
        timelineMarker: time,
      });
    }
  };

  // handleStopButton(): ì •ì§€ ë²„íŠ¼ í´ë¦­ í•¨ìˆ˜
  const handleStopButton = () => {
    sendPlaybackStatus({
      recording: isRecording,
      playState: "STOP",
      timelineMarker: 0,
    });
  };

  return (
    <section className="flex h-full max-h-16 w-full flex-grow-0 flex-row items-center justify-between border border-gray-300 px-16 py-[22px]">
      <div className="flex h-full flex-row items-center justify-center gap-x-4">
        <div onClick={handleRecording} className="cursor-pointer">
          <RecordButton width={20} height={20} />
        </div>
        <div onClick={handlePlayButton} className="cursor-pointer">
          {isPlaying ? (
            <PauseButton width={20} height={20} />
          ) : (
            <PlayButton width={20} height={20} />
          )}
        </div>
        <div onClick={handleStopButton} className="cursor-pointer">
          <StopButton width={20} height={20} />
        </div>
      </div>
      <div className="flex h-full flex-row items-center justify-center gap-x-3">
        <H4 className="text-white-100">{formatTime(time)}</H4>
        <H4 className="text-white-100">/</H4>
        <H4 className="text-white-100">{formatTime(duration)}</H4>
      </div>
      <div className="flex h-full items-center justify-center gap-x-4">
        <ShareButton />
        <RenderingButton
          videoUrl={videoUrl}
          tracks={tracks}
          setTracks={setTracks}
        />
        <StoreButton tracks={tracks} assets={assets} />
      </div>
    </section>
  );
};

export default PlayBar;
